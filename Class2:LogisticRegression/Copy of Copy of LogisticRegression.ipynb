{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of LogisticRegression.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"7X5NngwC8Wop","colab_type":"text"},"cell_type":"markdown","source":["# Wine Classification\n","In this dataset, instead of trying to predict the quality of the wine based on it's characteristics, we'll be trying to distinguish red wine vs. white wine. You should already have the red wine dataset, go into the class data folder for the white wine dataset.\n","\n","To upload your dataset, open up the menu on the left side of the screen, click the \"files\" tab, and click the upload icon."]},{"metadata":{"id":"UarmI3BTxm5F","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","A bunch of imports.\n","\n","The csv library is for reading \".csv\" files (things like spreadsheets)\n","keras is our main library for machine learning\n","numpy is short for number python, and it's a really nice library for all things\n","mathematical.\n","\n","Mattplotlib.pyplot is plotting software we'll be using.\n","'''\n","import csv\n","import keras\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ppKRl3Qz9CSQ","colab_type":"text"},"cell_type":"markdown","source":["## Processing Data"]},{"metadata":{"id":"3RgFl_T68_4V","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","This is a function I wrote to read csv files.\n","Understanding this isn't too important; please skip over it for now.\n","'''\n","def read_csv(file_name):\n","  data = []\n","  \n","  with open(file_name) as file:\n","    header = file.readline()\n","    \n","    def readline(string):\n","      string = string.replace('\"', '')\n","      string = string.replace('\\n', '')\n","      array = string.split(';')\n","      \n","      return array\n","    \n","    header = readline(header)\n","    \n","    for line in file:\n","      data_point = readline(line)\n","      data_point = [(float)(number) for number in data_point]\n","      data.append(data_point)\n","  \n","  return header, data\n","\n","def display(header, data):\n","  import pandas as pd\n","  import copy\n","  total_data = [[header[i] for i in range(len(header))]] + copy.deepcopy(data)\n","  df = pd.DataFrame(total_data)\n","  print(df.head())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xUNLdSv-qXKE","colab_type":"text"},"cell_type":"markdown","source":["Let's parse the data and understand it:"]},{"metadata":{"id":"lmPIxFxu9HJM","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Here's our data. white_wine is a 2D list:\n"," \n","[[a0, a1, a2, a3, a4, a5, ..., a11],\n"," [b0, b1, b2, b3, b4, b5, ..., b11],\n"," [c0, c1, c2, c3, c4, c5, ..., c11], ...]\n","\n","In this situation, each row [a0, a1, ..., a12] represents an example of a bottle of wine.\n","Each column a0, b0, c0, ..., etc. represents a \"feature\" or an aspect of that wine.\n","\n","For example, a0 represents the \"fixed acidity\" of the first example. b0 represents\n","the \"fixed acidity\" of the second example, etc.\n","'''\n","\n","header, white_wine = read_csv(\"white_wine.csv\")\n","header, red_wine = read_csv(\"red_wine.csv\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z3kYZXKT9NKJ","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Just displaying our data\n","'''\n","\n","display(header, white_wine)\n","#display(header, red_wine)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TnWNJe7o9UNa","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Now we concatenate the two datasets and make a corresponding \"answers\" list. \n","A value of 0 means \"not red wine\" and a value of 1 means \"red wine\"\n","'''\n","\n","'''\n","data is a 2D array containing all the elements of white_wine and red_wine\n","The order is preserved, so first it's the elements of white_wine and then the\n","elements of red_wine\n","'''\n","data = white_wine + red_wine\n","\n","'''\n","We make a corresponding answers array. The answer to data[i] is answers[i]. And\n","answers[i] is either [0] if the wine is white wine, and [1] if the wine is red wine.\n","'''\n","answers = [[0] for i in range(len(white_wine))] + [[1] for i in range(len(red_wine))]\n","\n","# cast to numpy arrays\n","data = np.array(data).astype(\"float64\")\n","answers = np.array(answers).astype(\"float64\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p-135bqI5bWx","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Now we normalize the data into the range 0-1. This ways something like fixed_acidity\n","(range 0-10) doesn't outweigh something like chlorides (range 0-0.1)\n","'''\n","\n","data = keras.utils.normalize(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eZINH-SXAd-j","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Now we just shuffle the arrays. Note that it only shuffles the elements one layer\n","deep. So if we have [[1, 2], [3, 4]], we might get [[3, 4], [1, 2]] but never\n","[[4, 3], [1, 2]].\n","\n","np.random.seed(0) makes sure that numpy shuffles our arrays in the same way.\n","Basically, a \"seed\" is a starting point for generating random numbers.\n","\n","So if you do np.random.seed(0) and then print(np.random.randint(0, 100)), it will\n","always be 44. \n","\n","Likewise, if we have x = [a1, a2, a3, a4] and we do np.random.seed(0) and np.random.shuffle(x)\n","Then x will always be transformed to [a3, a4, a2, a1].\n","'''\n","\n","np.random.seed(0)\n","np.random.shuffle(data)\n","\n","np.random.seed(0)\n","np.random.shuffle(answers)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5n5JxhCb-Uin","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Let's look at some pictures!\n","'''\n","\n","# Usually matplotlib uses Tkinter, which doesn't work on notebooks. This \"magic\n","# function\" with the % in front of it signals to not use Tkinter.\n","%matplotlib inline\n","\n","# We take the first 100 elements of our data and the corresponding 100 answers.\n","x_plot = data[0:100]\n","y_plot = answers[0:100]\n","\n","# Given the ith datapoint, we want to plot datapoint[idx] over 0 or 1 depending\n","# on if it's red wine or not. (1 is red wine)\n","idx = 9\n","\n","# Just going through our data and taking the (idx)th element of each datapoint.\n","# Again, if idx = 10, it's alcohol, if idx = 7 it's density, etc.\n","# (Look at the labels when we display the data)\n","data_plot = [data_point[idx] for data_point in x_plot]\n","\n","# Just creates a scatter plot. Regular plt.plot(data_plot, y_plot) gives you a\n","# line graph, which is bad.\n","plt.scatter(data_plot, y_plot)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ynaICwZQ_wnX","colab_type":"text"},"cell_type":"markdown","source":["## Building A Model"]},{"metadata":{"id":"g_zWwihR_1cU","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","We create a simple Sequential keras model. A Sequential object is the framework\n","for models in keras. Right now our Sequential model is empty, so it won't do\n","anything. We'll need to add things into it first.\n","'''\n","\n","model = keras.models.Sequential()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rg4_fb0t_5tz","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","A \"Dense\" object is keras' version of linear regression. But now, since we want\n","logistic regression, we need to apply the sigmoid function after the linear\n","regression.\n","\n","To do this, we set activation = \"sigmoid\"\n","'''\n","\n","# The activation function is what we do after we apply the weighted sum.\n","# Usually, we just have y = w0x0 + w1x1 + ... + w11x11 + b, but with the activation\n","# keras does y = sigmoid(w0x0 + w1x1 + ... + w11x11 + b) for us.\n","\n","# Of course, 'sigmoid' is preprogrammed; and there are a few others, but that's for later\n","log_regressor = keras.layers.Dense(1, input_shape=(12,), activation='sigmoid')\n","\n","# Now we add the logistic regression \"layer\" to our model. It might not make sense\n","# right now, but remember, keras is designed to do neural networks not just \n","# linear or logistic regression. The current keras design allows for generalizations\n","# of more than one \"layer\" in a model; we're just not doing any of that.\n","model.add(log_regressor)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fIj11heXAIUR","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","We need to compile a model before we run it. \"opt\" here is an optimizer, basically\n","a way of getting better. Here, we let opt = Adam, which stands for Adaptive momentum estimation.\n","(A Glorified version of Gradient Descent)\n","\n","lr here means \"learning rate.\" This is the step size of your gradient descent. \n","If the model isn't training fast enough, try increasing it. If the model is getting\n","worse, try decreasing it.\n","\n","Loss is mean squared error. This is what we try to minimize. \n","Instead, we can also use \"binary_crossentropy\" which might be better.\n","'''\n","opt = keras.optimizers.Adam(lr = 0.01)\n","model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mean_absolute_error'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DjPKuO12AUNo","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","The line\n","--history = model.fit(X, y, epochs = 100)--\n","stores the history of how the model learned in a variable called history\n","\n","(Look below for an explanation of the output.)\n","'''\n","history = model.fit(data, answers, epochs = 100)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6ZJMUNcdcfGX","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Explanation of the output:\n","\n","Epoch 24/100\n","6497/6497 [==============================] - 0s 36us/step - loss: 0.0185 - mean_absolute_error: 0.0308\n","\n","An epoch is a runthrough of the data. We can see that this is the 24th epoch out of the 100\n","which it plans to run.\n","\n","6497 is the total number of examples in our data. To finish an epoch, the model has \n","to train on all 6497 datapoints.\n","\n","loss: 0.0185 This is the average of the squared error for each datapoint. Note that\n","it uses squared error because we set loss = 'mean_squared_error' when compiling.\n","\n","mean_absolute_error: 0.0308. This is an error metric we can kinda understand.\n","On average, it's guesses are around 0.0308 off the right answer.\n","\n","So, for instance, if the bottle of wine is red wine, on average, it might gueses a 0.97\n","or a 97% chance that it's red wine.\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aVaVZzPqdCiy","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","But so far our model has been doing regression; it only becomes classification\n","once we decide how to classify things. For our purposes, we set the classification\n","threshold to be 0.5. If the bottle has a more than 50% chance of being red wine, we \n","classify it as red wine. If the bottle has less than a 50% chance of being red wine,\n","we classify it as white wine.\n","'''\n","\n","def classify(model, X):\n","  # First we run the data through the model to get the probabilities of being red wine\n","  y = model.predict(X)\n","  \n","  # np.rint here stands for \"round int\". Basically, it takes any decimal and rounds\n","  # to the nearest integer. So 0.75, for example, goes to 1.\n","  y_classify = np.rint(y)\n","  \n","  # We'll see very soon a visual representation of these results.\n","  return y_classify"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S1Jb8EImBsBV","colab_type":"text"},"cell_type":"markdown","source":["## Displaying Results\n","Time to see how we're doing!"]},{"metadata":{"id":"WX6AUuw-BxvA","colab_type":"code","colab":{}},"cell_type":"code","source":["# Again, this is the \"magic function\" which enables matplotlib to work in a notebook.\n","%matplotlib inline\n","\n","# We'll plot only the first 50 points; plotting all 6500 datapoints will make our\n","# graphs incomprehendible.\n","x_subset = data[0:50]\n","y_subset = answers[0:50]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pRt6aHUlB2_F","colab_type":"code","colab":{}},"cell_type":"code","source":["index1 = 5\n","\n","# Plot the predictions of the model vs the actual answers\n","y_guess = model.predict(x_subset)\n","\n","# Here, we extract the feature point which we want to plot against. if index1 = 0,\n","# for example, then we're plotting y against \"fixed_acidity\"\n","x_plot = [data_point[index1] for data_point in x_subset]\n","# Comprehension questions: why do we need to plot against x_plot, why not plot against x_subset?\n","\n","# Plot the guesses (blue)\n","plt.scatter(x_plot, y_guess, label = 'Guess')\n","# Plot the real values (green)\n","plt.scatter(x_plot, y_subset, label = 'Real Value')\n","# Label the plot and make a legend\n","plt.xlabel('Fixed Acidity')\n","plt.ylabel('Wine Quality')\n","plt.legend()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bpgcIjSoB6Xi","colab_type":"code","colab":{}},"cell_type":"code","source":["#You can change this around\n","index2 = 8\n","\n","# Plot the impact of the different features vs the actual answers\n","# We do this by setting all features equal to zero except for the one we're interested in\n","x_input = []\n","\n","for i in range(len(x_subset)):\n","  # Create an artificial datapoint with all values = 0\n","  data_point = [0 for i in range(12)]\n","  # Set the index2 element to not zero. This will isolate the impact that index2\n","  # gives us.\n","  data_point[index2] = x_subset[i][index2]\n","  x_input.append(data_point)\n","\n","# Convert to numpy array bc you can only predict on numpy arrays.\n","x_input = np.array(x_input)\n","\n","# Strip away all the features except for the one we're interested in.\n","# (Can't plot a 12-dimensional vector in a 2D plot)\n","x_plot = [x_input[i][index2] for i in range(len(x_input))]\n","\n","# Make guesses based on our skewed dataset\n","y_guess = model.predict(x_input)\n","\n","# Plot the guesses and the real values.\n","plt.scatter(x_plot, y_guess) # Guesses = blue\n","plt.scatter(x_plot, y_subset) # Real values = green\n","\n","# First note the sigmoid-shaped curve. Why is this?\n","# Second, does more \"fixed acidity\" mean white or red wine?"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2bBCNGnBgUiu","colab_type":"text"},"cell_type":"markdown","source":["### Plotting Classification\n","So far, we've only been plotting continuous values; those are easier to graph. Instead, now let's try to plot discrete values."]},{"metadata":{"id":"eK74P1kzf6TH","colab_type":"code","colab":{}},"cell_type":"code","source":["# Now we can compare our guesses with our answers!\n","# classify is a function, written above which rounds our probabilities to the\n","# nearest number. The rounding makes logistic regression \"classification\".\n","guesses = classify(model, data)\n","\n","# Let's just try comparing a few of them.\n","print(guesses[0:5])\n","print(answers[0:5])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xK_iQ63JhL5t","colab_type":"code","colab":{}},"cell_type":"code","source":["# Hopefully you're model's doing well!\n","# Now let's try to plot results over the whole dataset.\n","\n","ww = 0 #Stands for: white wine, guessed white wine.\n","rr = 0 #Stands for: red wine, guessed red wine.\n","wr = 0 #Stands for: white wine, guessed red wine.\n","rw = 0 #Stands for: red wine, guessed white wine.\n","\n","for i in range(len(answers)):\n","  if answers[i][0] == 0 and guesses[i][0] == 0:\n","    ww += 1\n","  elif answers[i][0] == 1 and guesses[i][0] == 1:\n","    rr += 1\n","  elif answers[i][0] == 0 and guesses[i][0] == 1:\n","    wr += 1\n","  else:\n","    rw += 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MQ42NgSliqew","colab_type":"code","colab":{}},"cell_type":"code","source":["# Let's plot the confusion matrix!\n","\n","print(\"        Guesses:   White Wine    Red Wine\")\n","print(\"Truth\")\n","print(\"White Wine:           {}            {}\".format(ww, wr))\n","print(\"Red Wine:             {}            {}\".format(rw, rr))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uKyjmWpsj3hG","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Hopefully things are doing well! This matrix is very useful to plot, and can be \n","extended to multi-class classification.\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dd_65Zuwup2f","colab_type":"text"},"cell_type":"markdown","source":["# Breast Cancer Classification\n","Now try applying implementing it yourself!"]},{"metadata":{"id":"TOqdZbIzw_wY","colab_type":"text"},"cell_type":"markdown","source":["## Processing Data"]},{"metadata":{"id":"28BgpyKRvZWz","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Sklearn is just another machine learning library. We're just stealing it's dataset.\n","'''\n","\n","from sklearn import datasets\n","\n","breast_cancer = datasets.load_breast_cancer()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FvD0uSwgwgzo","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Let's see what sklearn says about the dataset.\n","'''\n","print(breast_cancer['DESCR'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mYOyLtlJw01v","colab_type":"code","colab":{}},"cell_type":"code","source":["x_bc = breast_cancer['data']\n","y_bc = breast_cancer['target']\n","\n","# We need to normalize our data.\n","x_bc = keras.utils.normalize(x_bc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NfCQ4zD9xESf","colab_type":"text"},"cell_type":"markdown","source":["## Building A Model\n","Try building your own model! If you have questions, try refering to the \"Building a Model\" section in the wine classification step. If you have any more questions, ask Joshua or Oliver!\n","\n","Disclaimer: keras is a library which does keep many things \"under the hood.\" You just need to follow it's rules, and it'll magically do the Gradient descent stuff for you. The lecture is meant to teach you the intuition behind keras' actions, but the implementation time is meant to show how exactly do you use keras.\n","\n","The keras documentation is here: [keras.io](https://keras.io)"]},{"metadata":{"id":"XlaV0ZqNx4Ui","colab_type":"code","colab":{}},"cell_type":"code","source":["# Try building your own model!"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RPcPcgFfyI10","colab_type":"text"},"cell_type":"markdown","source":["## Displaying Results\n","Now, display whichever results are useful to you. We recommend printing out your confusion matrix first."]},{"metadata":{"id":"Bxpnf3eLyIFy","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}