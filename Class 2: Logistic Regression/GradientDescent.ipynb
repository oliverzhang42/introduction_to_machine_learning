{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GradientDescent.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"jBAGA1RDklRz","colab_type":"text"},"cell_type":"markdown","source":["# Implementing Gradient Descent by Yourself!\n","This notebook aims to give you a sense of how to do Gradient Descent by yourself. "]},{"metadata":{"id":"YHC8NpNplKzM","colab_type":"text"},"cell_type":"markdown","source":["## The Data"]},{"metadata":{"id":"F9NU_szrkgTn","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","We have some dataset, and we're trying to find the line of best fit.\n","'''\n","\n","x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","y = [-1.0, 2.5, 4.0, 5.5, 5.0, 6.5, 10.0, 7.5, 13.0, 14.5]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PHWfS5T2lp4L","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Plotting the dataset\n","'''\n","\n","import matplotlib.pyplot as plt\n","\n","# This is a \"magic function\" specific to python notebooks which allow us to use\n","# matplotlib. Matplotlib is based on Tkinter, which usually opens up new windows.\n","# But this can't happen in python notebooks, so we need to tell it to do something different.\n","%matplotlib inline\n","\n","# A small quirk of matplotlib; it can't plot [1, 2, 3]; the data must be [[1], [2], [3]]\n","x_plot = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]\n","plt.scatter(x_plot, y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NbplumMBmWgi","colab_type":"text"},"cell_type":"markdown","source":["## The Model"]},{"metadata":{"id":"R5yX86_pmVf2","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Let's define some model! We'll have the equation y = wx + b, and w and b will start at 0.\n","'''\n","\n","w = 0\n","b = 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rnvC032xmg92","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Try creating a function which inputs a list and uses your model to output predictions.\n","Call this function \"predict\"\n","'''\n","\n","def predict(data, w_, b_):\n","  pass"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X8NT0w6ynSiy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"10bf86ca-c5bf-4888-fae3-5b6d7ea65b26","executionInfo":{"status":"ok","timestamp":1541383467465,"user_tz":480,"elapsed":287,"user":{"displayName":"Oliver Zhang","photoUrl":"https://lh6.googleusercontent.com/-hSQdak-2LGo/AAAAAAAAAAI/AAAAAAAAAFg/5cl0NDg7JlQ/s64/photo.jpg","userId":"02443409330296794886"}}},"cell_type":"code","source":["'''\n","Also try creating a function for calculating loss. Given two lists, try finding \n","the loss between them.\n","\n","You can either use the mean squared error or mean absolute error, your choice.\n","'''\n","\n","def loss(answers, guesses):\n","  pass"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["''"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"9APsRHTBxa27","colab_type":"text"},"cell_type":"markdown","source":["## Gradient Descent Version 1"]},{"metadata":{"id":"EIWdH_1wmyaN","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Try doing naive gradient descent. Basically, at each step, see if w + 0.01 or w - 0.01 minimizes loss.\n","Also, try seeing if b + 0.01 or b - 0.01 minimizes loss.\n","'''\n","\n","# In this case, learning rate (lr) = 0.01. Feel free to modify this!\n","lr = 0.01\n","training_steps = 10000\n","\n","for i in range(training_steps):\n","  pass\n","  # Case 1.1: (w + 0.01), b. Calculate the predictions and the corresponding loss.\n","  # Case 1.2: (w - 0.01), b. Calculate the predictions and the corresponding loss.\n","  \n","  # Case 2.1: w, (b + 0.01). Calculate the predictions and the corresponding loss.\n","  # Case 2.2: w, (b - 0.01). Calculate the predictions and the corresponding loss. \n","  \n","  # Finally, update w and b."],"execution_count":0,"outputs":[]},{"metadata":{"id":"yH2IpGuUxIes","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Let's try to see how well our model is doing.\n","'''\n","\n","y_pred = predict(x, w, b)\n","\n","plt.scatter(x_plot, y)\n","plt.scatter(x_plot, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dLjJBXKTxWHv","colab_type":"text"},"cell_type":"markdown","source":["## Gradient Descent Version 2\n","This previous approach is good enough for us right now. However, in the future, we'll be trying to update many, many weights in a very complex model. So let's see how to update the weights, given the derivative of the loss with respect to a weight."]},{"metadata":{"id":"sUCy1TX-yqBp","colab_type":"text"},"cell_type":"markdown","source":["Consider our loss function:\n","\n","$$loss(w, b) = \\frac{1}{m}\\sum\\limits_{i = 0}^{m-1}(y_i - y_{i pred})^2 = \\frac{1}{m}\\sum\\limits_{i = 0}^{m-1}(y_i - wx_i - b)^2$$\n","\n","Let's try to take the derivative of loss($w$, $b$) with respect to $w$. Want to find:\n","\n","$$\\frac{dloss(w,b)}{dw}$$\n","\n","Note that $y_i$ and $x_i$ are constants. For our purposes, when taking the derivative with respect to $w$, the variable $b$ can be treated as a constant as well.\n","\n","Now let's consider one term in the summation:\n","\n","$$\\frac{d(y_i - wx_i - b)^2}{dw}$$\n","\n","Try computing this derivative! You'll need the chain rule.\n","\n","Also try to compute \n","\n","$$\\frac{dloss(w,b)}{db}$$"]},{"metadata":{"id":"_vykGfNiz0bi","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","We define two derivative functions.\n","'''\n","\n","# The derivative of our loss function with respect to w\n","def derivative_w(outputs, inputs, w, b): \n","  pass\n","  \n","def derivative_b(outputs, inputs, w, b):\n","  pass"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kbSLxEt40SKl","colab_type":"code","colab":{}},"cell_type":"code","source":["# Finally, let's try updating with our derivatives.\n","\n","learning_rate = 0.01\n","steps = 1000\n","\n","# Reset the w and b\n","w = 0\n","b = 0\n","\n","# Given your derivatives work, this should work as well.\n","for i in range(steps):\n","  # First, we calculate the corresponding derivatives\n","  der_w = derivative_w(y, x, w, b)\n","  der_b = derivative_b(y, x, w, b)\n","  \n","  # Second, we update. If the slope (der_w) is positive, then we go backwards\n","  # if the slope is negative, we go forwards. Also, we multiply by learning rate\n","  # to control how fast/slow we're going.\n","  w += -learning_rate * der_w\n","  b += -learning_rate * der_b\n","  \n","  # If you're having trouble visualizing this, ask Oliver or Joshua, and we'll\n","  # go over the 3D geometry behind this."],"execution_count":0,"outputs":[]},{"metadata":{"id":"WipbjK9g1OaM","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Let's try to see how well our model is doing.\n","'''\n","\n","y_pred = predict(x, w, b)\n","\n","plt.scatter(x_plot, y)\n","plt.scatter(x_plot, y_pred)"],"execution_count":0,"outputs":[]}]}